# -*- coding: utf-8 -*-
"""predictive_modelling_XGB

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GVpYkvS3_loB6bKbfQuUAiA7NKPPOTFY
"""

from google.colab import drive

drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

df = pd.read_excel("/content/drive/MyDrive/JDA/Predictive Modelling - Data file.xlsx",sheet_name = "train")

df['order_date_year'] = df['order_date'].dt.year
df['order_date_month'] = df['order_date'].dt.month
df['order_date_day'] = df['order_date'].dt.day
df['order_date_day_of_week'] = df['order_date'].dt.dayofweek

#removing redundant and unnecessary columns
df.drop(columns = ['id','order_day_of_week'],inplace = True)

from datetime import datetime
df.loc[df['order_date'] == df['forecast_start_date']].shape   #indicates order date is same as forecast_start_date for all the records
df['forecast_period'] = (df['forecast_end_date'] - df['forecast_start_date']).dt.days

#dropping date field columns
df.drop(columns = ['order_date','forecast_start_date','forecast_end_date'],inplace = True)

cat_cols = ['store','supplier_id','product_id','category',]

df[cat_cols] = df[cat_cols].astype('category')

sample1_df = df.copy()

sample1_df['on_hand_quantity'] = sample1_df['on_hand_quantity'].apply(lambda x: 0 if x < 0 else x)

# Taking 4k records (2000 each from store )for test data 
sample1_s1_test = sample1_df.loc[sample1_df["store"] == 's1'].iloc[-2000:,]
sample1_s2_test = sample1_df.loc[sample1_df["store"] == 's2'].iloc[-2000:,]

sample_test = pd.concat([sample1_s1_test,sample1_s2_test])

sample_train = sample1_df[ ~sample1_df.isin(sample_test)].dropna()

sample_x_train = sample_train.drop(['actual_order_quantity'],axis = 1)
sample_y_train = sample_train['actual_order_quantity']
sample_x_test = sample_test.drop(['actual_order_quantity'],axis = 1)
sample_y_test = sample_test['actual_order_quantity']

print(sample_x_train.shape)
print(sample_y_train.shape)
print(sample_x_test.shape)
print(sample_y_test.shape)

num_cols = list(set(df.columns.tolist()) - set(cat_cols))
input_num_cols = [x for x in num_cols if x!= "actual_order_quantity"]

from sklearn.preprocessing import StandardScaler

#Standarsing and encoding
scaler2 = StandardScaler()
sample_x_train[input_num_cols]  = scaler2.fit_transform(sample_x_train[input_num_cols])
sample_x_test[input_num_cols]  = scaler2.transform(sample_x_test[input_num_cols])

!pip install category_encoders

import category_encoders as ce

cat_cols_for_encoding = [x for x in cat_cols if x != "store"]

# as there are only two levels in store not using binary encoder
sample_x_train["store"] = sample_x_train["store"].map({'s1':0 ,'s2':1})
sample_x_test["store"] = sample_x_test["store"].map({'s1':0 ,'s2':1})

binaryEncoder = ce.BinaryEncoder(cols= cat_cols_for_encoding,return_df=True)
sample_x_train_cat = binaryEncoder.fit_transform(sample_x_train[cat_cols_for_encoding])

sample_x_test_cat = binaryEncoder.transform(sample_x_test[cat_cols_for_encoding])

sample_x_train.drop(cat_cols_for_encoding,axis = 1,inplace=True)
sample_x_test.drop(cat_cols_for_encoding,axis = 1,inplace=True)
sample_x_train_final = sample_x_train.join(sample_x_train_cat)
sample_x_test_final = sample_x_test.join(sample_x_test_cat)

sample_x_train_final['store'] = pd.to_numeric( sample_x_train_final['store'])
 sample_x_test_final['store'] = pd.to_numeric( sample_x_test_final['store'])

from sklearn.model_selection import GridSearchCV

import xgboost as xgb
xgb1 = xgb.XGBRegressor()
parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [.01,.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7,10],
              'min_child_weight': [4],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [100]}

xgb_grid = GridSearchCV(xgb1,
                        parameters,
                        cv = 10,
                        n_jobs = -1,
                        scoring='neg_root_mean_squared_error')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# xgb_grid.fit(sample_x_train_final,sample_y_train)

xgb_grid.best_params_

y_train_xgb_pred = xgb_grid.best_estimator_.predict(sample_x_train_final)
y_test_xgb_pred = xgb_grid.best_estimator_.predict(sample_x_test_final)

#calculating rmse
from sklearn.metrics import mean_squared_error

mean_squared_error(sample_y_train,y_train_xgb_pred, squared=False)

mean_squared_error(sample_y_test,y_test_xgb_pred, squared=False)

y_train_xgb_pred.shape

new_df = sample_train.copy()

new_df.head()

new_df['pred_train'] = y_train_xgb_pred



new_df[['actual_order_quantity','pred_train']].plot()

new_test_df = sample_test.copy()

new_test_df['pred_test'] = y_test_xgb_pred



new_test_df.head()

new_test_df[['actual_order_quantity','pred_test']].plot()

